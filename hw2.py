# -*- coding: utf-8 -*-
"""HW2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gax_NlJtq0UoP0ZlVbYDxqmkLQeov-I9

# HW 2
## CS486 - Ethical and Trustworthy AI Fall 2025
## September 26, 2025
## Smrithi Panuganti (spanuganti@dons.usfca.edu)

## Audit of Original Data
"""

import pandas as pd                 # data manipulation
import numpy as np                  # numerical operations
import matplotlib.pyplot as plt     # basic plotting
import seaborn as sns               # statistical viz
# Import necessary modules from scikit-learn for model building and evaluation
from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report

sns.set(color_codes=True)           # allow for use of shorthand color codes

# Import necessary libraries for Google Drive interaction and file path manipulation
from google.colab import drive
import os

# Mount Google Drive to access files
drive.mount('/content/drive', force_remount=True)

# Define the project folder path on Google Drive
project_folder = "/content/drive/My Drive/EthicalAI"

# Construct the full file path to the dataset
file_path = os.path.join(project_folder, "Original training DB e1 positive.csv")

# Read the CSV file into a pandas DataFrame
df = pd.read_csv(file_path)

"""## Audit of Original Data"""

print(df.shape)
df.head(5)

# Assign the DataFrame to a variable named data
data = df

# Print the shape of the data (number of rows and columns)
print("Data shape:", data.shape)
# Print the number of columns
print("Columns:", len(data.columns))
# Print descriptive statistics of the data
print(data.describe())
# Print the counts of each unique value in the "Label" column
print(data["Label"].value_counts())

# Display concise information about the DataFrame, including data types and non-null values
data.info()

"""## Creation of training DB and verification DB"""

# Select 1 positive and 1 negative sample randomly for a Verification DB
pos_sample = data[data["Label"] == 1].sample(1, random_state=42)
neg_sample = data[data["Label"] == 0].sample(1, random_state=42)
verification_db = pd.concat([pos_sample, neg_sample])

# Remove the selected samples from the original data to create the Training DB
train_db = data.drop(verification_db.index)

# Print the shapes of the Training and Verification DataFrames
print("Training DB shape:", train_db.shape)
print("Verification DB shape:", verification_db.shape)

# Separate features (X) and labels (y) for the Training DB
X = train_db.drop("Label", axis=1)
y = train_db["Label"]

# Display the Training DB
print("Training DB describe: \n")
train_db

# Display the Verification DB
print("Verification DB describe: \n")
verification_db

# Get the counts of each class in the "Label" column
class_counts = data["Label"].value_counts()
# Print the class counts
print(class_counts)

# Calculate and print the relative proportion of each class
print("Class distribution (%):")
print(class_counts / len(data) * 100)

# Plot the class distribution as a bar plot
class_counts.plot(kind="bar", color=["skyblue", "salmon"])
# Add title and labels to the plot
plt.title("Class Distribution")
plt.xlabel("Class")
plt.ylabel("Number of Samples")
plt.xticks(rotation=0)
# Display the plot
plt.show()

"""## Experimental Methods and Setup"""

# Define the hyperparameter grid for the Random Forest model
param_grid = {
    "n_estimators": [500, 1000, 2000],  # Number of trees in the forest
    "max_features": ["sqrt", "log2", 0.2, 0.5]  # Number of features to consider when looking for the best split
}
# Print the hyperparameter grid
print(param_grid)

"""## Actual Results of RF Training and Accuracy Estimates"""

# Initialize StratifiedKFold for cross-validation, ensuring folds have similar class distribution
skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# Initialize variables to store the best model, score, and parameters
rf_model = None
score = 0
params = {}

# Iterate through hyperparameter combinations
for n in param_grid["n_estimators"]:
    for m in param_grid["max_features"]:
        # Initialize a Random Forest Classifier with current hyperparameters
        rf = RandomForestClassifier(n_estimators=n, max_features=m, random_state=42, n_jobs=-1)
        # Perform cross-validation and get predicted labels
        preds = cross_val_predict(rf, X, y, cv=skf)
        # Calculate accuracy
        acc = accuracy_score(y, preds)
        # Check if current accuracy is better than the best found so far
        if acc > score:
            # Update best score, model, and parameters
            score = acc
            rf_model = rf
            params = {"n_estimators": n, "max_features": m}

# Print the best hyperparameters and the corresponding cross-validation accuracy
print("Best Parameters:", params)
print("Best Cross-Validation Accuracy (3-fold):", f"{score:.5f}")

# Retrain the best model on the full training data
rf_model.fit(X, y)

# Evaluate the retrained model with cross-validation for detailed metrics
y_pred = cross_val_predict(rf_model, X, y, cv=skf)

# Calculate various evaluation metrics
cm = confusion_matrix(y, y_pred)
acc = accuracy_score(y, y_pred)
prec = precision_score(y, y_pred)
rec = recall_score(y, y_pred)
f1 = f1_score(y, y_pred)

# Print the evaluation metrics
print("Confusion Matrix:\n", cm)
print(f"Accuracy: {acc:.5f}")
print(f"Precision: {prec:.5f}")
print(f"Recall: {rec:.5f}")
print(f"F1 Score: {f1:.5f}")

# Plot the confusion matrix using a heatmap
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Class 0","Class 1"], yticklabels=["Class 0","Class 1"])
# Add title and labels to the plot
plt.title("Confusion Matrix (3-Fold CV)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
# Display the plot
plt.show()

"""## Feature Ranking"""

# Calculate feature importances from the trained Random Forest model
importances = pd.Series(rf_model.feature_importances_, index=X.columns)
# Get the top 10 most important features
top10 = importances.sort_values(ascending=False).head(10)

# Print the top 10 features and their importances
print("Top 10 Features:")
print(top10)

# Create a figure for the plot
plt.figure(figsize=(10,6))
# Plot the top 10 feature importances as a bar plot
top10.plot(kind="bar")
# Add title and labels to the plot
plt.title("Top 10 Important Features (Gini Importance)")
plt.ylabel("Importance")
# Display the plot
plt.show()

"""## RF Run Time test"""

# Separate features (X) and labels (y) for the Verification DB
X_ver = verification_db.drop("Label", axis=1)
y_ver = verification_db["Label"]

# Make predictions on the Verification DB using the trained model
ver_preds = rf_model.predict(X_ver)
# Get the probability estimates for each class on the Verification DB
ver_probs = rf_model.predict_proba(X_ver)

# Create a DataFrame to display the verification results
results = pd.DataFrame({
    "True Label": y_ver.values,
    "Predicted Label": ver_preds,
    "Probability Class 0": ver_probs[:,0],
    "Probability Class 1": ver_probs[:,1]
})

# Print the verification results
print("Verification Results:")
print(results)