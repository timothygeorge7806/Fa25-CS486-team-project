# -*- coding: utf-8 -*-
"""Team3FinalProject

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gax_NlJtq0UoP0ZlVbYDxqmkLQeov-I9

# Team 3 Final Project
## CS486 - Ethical and Trustworthy AI Fall 2025
###### Timothy George (tmgeorge2@dons.usfca.edu), Smrithi Panuganti (spanuganti@dons.usfca.edu), David Shubov (drshubov@dons.usfca.edu)

## Audit of Original Data
"""

import pandas as pd                 # data manipulation
import numpy as np                  # numerical operations
import matplotlib.pyplot as plt     # basic plotting
import seaborn as sns               # statistical viz
# Import necessary modules from scikit-learn for model building and evaluation
from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report

sns.set(color_codes=True)           # allow for use of shorthand color codes

import os


# Construct the full file path to the dataset
file_path = os.path.join("data/heart_disease_uci_cleaned.csv")

# Read the CSV file into a pandas DataFrame
df = pd.read_csv(file_path)

print(df.shape)
df.head(5)

# Assign the DataFrame to a variable named data
data = df

# Print the shape of the data (number of rows and columns)
print("Data shape:", data.shape)
# Print the number of columns
print("Columns:", len(data.columns))
# Print descriptive statistics of the data
print(data.describe())
# Print the counts of each unique value in the "Label" column
print(data["num"].value_counts())

# Display concise information about the DataFrame, including data types and non-null values
data.info()

"""## Creation of training DB and verification DB"""

data = df.copy()
# Handle categorical variables for the heart disease dataset
categorical_cols = [
    "sex", "cp", "fbs", "restecg",
    "exang", "slope", "ca", "thal"
]
# Clean and encode categorical and numeric columns properly
for col in categorical_cols:
    if col in data.columns:
        data[col] = data[col].astype(str).str.strip()  # ensure clean strings

# Detect non-numeric columns that are *not* in categorical_cols
non_numeric = [c for c in data.columns if data[c].dtype == 'object' and c not in categorical_cols]

# Try converting these to numeric (e.g. if 'ca' or 'thal' have '?')
for c in non_numeric:
    data[c] = pd.to_numeric(data[c], errors='coerce')

# Drop rows that became NaN after coercion (invalid numbers)
data = data.dropna()

# One-hot encode categorical features
data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True)

print("Categorical columns after update:", categorical_cols)
print("Shape of data_encoded after one-hot encoding:", data_encoded.shape)
print("First 5 rows of data_encoded:\n", data_encoded.head())

# Select 1 positive and 1 negative sample randomly for a Verification DB
pos_sample = data_encoded[data_encoded["num"] == 1].sample(1, random_state=42)
neg_sample = data_encoded[data_encoded["num"] == 0].sample(1, random_state=42)
verification_db = pd.concat([pos_sample, neg_sample])

# Remove the selected samples from the original data to create the Training DB
train_db = data_encoded.drop(verification_db.index)

# Print the shapes of the Training and Verification DataFrames
print("Training DB shape:", train_db.shape)
print("Verification DB shape:", verification_db.shape)

# Separate features (X) and labels (y) for the Training DB
X = train_db.drop("num", axis=1)
y = train_db["num"]

# Display the Training DB
print("Training DB describe: \n")
train_db

"""**Reasoning**:
The previous step successfully created the `train_db`, `verification_db`, `X`, and `y` DataFrames. Now, as per the instructions, I need to execute cell `4diGB_4wHy4X` to display the `verification_db` DataFrame.


"""

# Display the Verification DB
print("Verification DB describe: \n")
verification_db

# Get the counts of each class in the "num" column
class_counts = data["num"].value_counts()
# Print the class counts
print(class_counts)

# Calculate and print the relative proportion of each class
print("Class distribution (%):")
print(class_counts / len(data) * 100)

# Plot the class distribution as a bar plot
class_counts.plot(kind="bar", color=["skyblue", "salmon"])
# Add title and labels to the plot
plt.title("Class Distribution")
plt.xlabel("Class")
plt.ylabel("Number of Samples")
plt.xticks(rotation=0)
# Display the plot
plt.show()

"""## Experimental Methods and Setup"""

# Define the hyperparameter grid for the Random Forest model
param_grid = {
    "n_estimators": [500, 1000, 2000],  # Number of trees in the forest
    "max_features": ["sqrt", "log2", 0.2, 0.5]  # Number of features to consider when looking for the best split
}
# Print the hyperparameter grid
print(param_grid)

"""## Actual Results of RF Training and Accuracy Estimates"""

# Initialize StratifiedKFold for cross-validation, ensuring folds have similar class distribution
skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# Initialize variables to store the best model, score, and parameters
rf_model = None
score = 0
params = {}

# Iterate through hyperparameter combinations
for n in param_grid["n_estimators"]:
    for m in param_grid["max_features"]:
        # Initialize a Random Forest Classifier with current hyperparameters
        rf = RandomForestClassifier(n_estimators=n, max_features=m, random_state=42, n_jobs=-1)
        # Perform cross-validation and get predicted labels
        preds = cross_val_predict(rf, X, y, cv=skf)
        # Calculate accuracy
        acc = accuracy_score(y, preds)
        # Check if current accuracy is better than the best found so far
        if acc > score:
            # Update best score, model, and parameters
            score = acc
            rf_model = rf
            params = {"n_estimators": n, "max_features": m}

# Print the best hyperparameters and the corresponding cross-validation accuracy
print("Best Parameters:", params)
print("Best Cross-Validation Accuracy (3-fold):", f"{score:.5f}")

# Retrain the best model on the full training data
rf_model.fit(X, y)

# Evaluate the retrained model with cross-validation for detailed metrics
y_pred = cross_val_predict(rf_model, X, y, cv=skf)

# Calculate various evaluation metrics
cm = confusion_matrix(y, y_pred)
acc = accuracy_score(y, y_pred)
prec = precision_score(y, y_pred, average='weighted')  # supports multi-class
rec = recall_score(y, y_pred, average='weighted')
f1 = f1_score(y, y_pred, average='weighted')

# Print the evaluation metrics
print("Confusion Matrix:\n", cm)
print(f"Accuracy: {acc:.5f}")
print(f"Precision: {prec:.5f}")
print(f"Recall: {rec:.5f}")
print(f"F1 Score: {f1:.5f}")

# Plot the confusion matrix using a heatmap
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=np.unique(y), yticklabels=np.unique(y))
# Add title and labels to the plot
plt.title("Confusion Matrix (3-Fold CV)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
# Display the plot
plt.show()

"""## Feature Ranking"""
import pandas as pd
import matplotlib.pyplot as plt # Ensure matplotlib is imported for plotting

importances = pd.Series(rf_model.feature_importances_, index=X.columns)

# Define the list of original categorical features that were one-hot encoded
# This list comes from cell 88f77f4a, which was already executed.
categorical_cols = [
    "sex", "cp", "fbs", "restecg",
    "exang", "slope", "ca", "thal"
]

# Initialize a dictionary to store aggregated importances
aggregated_importances_dict = {}

# Iterate through the calculated importances
for feature, importance_value in importances.items():
    found_original = False
    for original_cat_col in categorical_cols:
        # Check if the current feature name starts with an original categorical column name followed by '_'
        if feature.startswith(original_cat_col + '_'):
            # Aggregate the importance under the original categorical column name
            aggregated_importances_dict[original_cat_col] = aggregated_importances_dict.get(original_cat_col, 0) + importance_value
            found_original = True
            break
    if not found_original:
        # If it's not a part of a one-hot encoded feature, it's a direct numerical feature
        aggregated_importances_dict[feature] = aggregated_importances_dict.get(feature, 0) + importance_value

# Convert the dictionary to a pandas Series for easy sorting and plotting
aggregated_importances = pd.Series(aggregated_importances_dict)

# Get the top 10 most important features from the aggregated importances
top10 = aggregated_importances.sort_values(ascending=False).head(10)

# Print the top 10 features and their importances
print("Top 10 Features (Aggregated):")
print(top10)

# Create a figure for the plot
plt.figure(figsize=(10,6))
# Plot the top 10 feature importances as a bar plot
top10.plot(kind="bar")
# Add title and labels to the plot
plt.title("Top 10 Important Features (Gini Importance)")
plt.ylabel("Importance")
# Display the plot
plt.show()

"""## RF Run Time Test"""

# Separate features (X) and labels (y) for the Verification DB
X_ver = verification_db.drop("num", axis=1)
y_ver = verification_db["num"]

# Make predictions on the Verification DB using the trained model
ver_preds = rf_model.predict(X_ver)
# Get the probability estimates for each class on the Verification DB
ver_probs = rf_model.predict_proba(X_ver)

# Create a DataFrame to display the verification results
results = pd.DataFrame({
    "True Label": y_ver.values,
    "Predicted Label": ver_preds,
    "Probability Class 0": ver_probs[:,0],
    "Probability Class 1": ver_probs[:,1]
})

# Print the verification results
print("Verification Results:")
print(results)
